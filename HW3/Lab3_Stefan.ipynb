{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <p style=\"text-align:center\">EE 379K: Lab Three</p>\n",
    "# <p style=\"text-align:center\">Kyle Grier and Stefan Bordovsky</p>\n",
    "#### <p style=\"text-align:center\">Due: Monday, 9/25, 3:00pm</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <b>Problem 1: Linear Algebra in Python.</b>  You can use all Python functions to solve this problem.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "1.  Consider the linear subspace S = span{v1,v2,v3,v4} where v1 = [1,2,3,4], v2 = [0,1,0,1], v3 =[1,4,3,6], v4 = [2,11,6,15].  Create a vector inside S different from v1,v2,v3,v4.  Create a vector not in S.  How would you check if a new vector is in S?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import scipy.linalg as ln\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vector inside span S: \n",
      "[ 2  6  6 10]\n",
      "Rank of matrix V = [v1, v2, v3, v4]: \n",
      "2\n",
      "Rank of new matrix with vector outside the span of S: \n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "v1 = [1, 2, 3, 4]\n",
    "v2 = [0, 1, 0, 1]\n",
    "v3 = [1, 4, 3, 6]\n",
    "v4 = [2, 11, 6, 15]\n",
    "\n",
    "# To find a vector inside span S = span{v1, v2, v3, v4}, we just take a linear combination of vectors in S (e.g. v1, v3)\n",
    "v_inside_S = np.add(v1, v3)\n",
    "print(\"A vector inside span S: \")\n",
    "print(v_inside_S)\n",
    "\n",
    "V = np.matrix([v1, v2, v3, v4])\n",
    "V = np.transpose(V)\n",
    "\n",
    "# To find if a vector is outside the span S = span{v1, v2, v3, v4}, we can check if the rank of matrix V = {v1, v2, v3, v4}\n",
    "#    is one less than the rank of matrix V_new = {v1, v2, v3, v4, v_outside_s_guess}\n",
    "v_outside_s_guess = [1, 1, 1, 1]\n",
    "V_new = np.matrix([v1, v2, v3, v4, v_outside_s_guess])\n",
    "V_new = np.transpose(V_new)\n",
    "\n",
    "V_rank = np.linalg.matrix_rank(V)\n",
    "V_new_rank = np.linalg.matrix_rank(V_new)\n",
    "# V_rank = 2, since V spans R2\n",
    "print(\"Rank of matrix V = [v1, v2, v3, v4]: \")\n",
    "print(str(V_rank))\n",
    "# V_rank = 3, meaning we v_outside_s_guess is outside the span S.\n",
    "print(\"Rank of new matrix with vector outside the span of S: \")\n",
    "print(str(V_new_rank))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "2. Find the dimension of the subspace S.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of subspace S = R2\n"
     ]
    }
   ],
   "source": [
    "# The dimension of subspace S is equal to the rank of the matrix V = {v1, v2, v3, v4}\n",
    "print(\"Dimension of subspace S = R\" + str(V_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "3. Find an orthonormal basis for the subspace S.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthonormal basis for subspace S = \n",
      "[[-0.1098991   0.2669373 ]\n",
      " [-0.54396107 -0.53613402]\n",
      " [-0.32969731  0.80081189]\n",
      " [-0.76375927 -0.00225943]]\n"
     ]
    }
   ],
   "source": [
    "V_basis = ln.orth(V)\n",
    "print(\"Orthonormal basis for subspace S = \")\n",
    "print(V_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "4. Solve the optimization problem $min_{x \\in S}\\|x - z^*\\|_2$ where $z^* = [1, 0, 0, 0]$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector x in S that minimizes the l2 norm of x - z*: \n",
      "[ 0.08333333 -0.08333333  0.25        0.08333333]\n"
     ]
    }
   ],
   "source": [
    "A = V_basis\n",
    "At = np.transpose(V_basis)\n",
    "AtA_inv = inv(np.matmul(At, A))\n",
    "A_AtA_inv_At = np.matmul(np.matmul(A, AtA_inv), At)\n",
    "b = [1, 0, 0, 0]\n",
    "min_x = np.matmul(A_AtA_inv_At, b)\n",
    "print(\"The vector x in S that minimizes the l2 norm of x - z*: \")\n",
    "print(min_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Scraping, Entropy and ICML papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>1. What are the top 10 common words in the ICML papers?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbordovsky/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /home/sbordovsky/anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'none' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-f6bb1c0a8a96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'none' is not defined"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import *\n",
    "\n",
    "# Scrape urls for all non-supplementary ICML papers from 2017.\n",
    "url = 'http://proceedings.mlr.press/v70/'\n",
    "html = urllib.urlopen(url).read()\n",
    "page_text = r.text.encode('utf-8').decode('ascii', 'ignore')\n",
    "page_soupy = BeautifulSoup.BeautifulSoup(page_text)\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "tags = soup('a')\n",
    "\n",
    "for tag in tags:\n",
    "    print tag.get('href', None).encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#Linux script for scraping pdf URLS from ICML 2017 and getting rid of URLs from supplementary papers.\n",
    "\n",
    "#Scrape pdf URLS\n",
    "rm /home/sbordovsky/Notes/EE\\ 379K/UT-Data-Science-Lab/HW3/pdf_urls.txt\n",
    "python bSoupScrape.py | grep -v 'supp' | grep --line-buffered '.pdf$' > /home/sbordovsky/Notes/EE\\ 379K/UT-Data-Science-Lab/HW3/pdf_urls.txt\n",
    "\n",
    "# Download pdfs\n",
    "cd /home/sbordovsky/Notes/EE\\ 379K/UT-Data-Science-Lab/HW3/pdfs\n",
    "cat /home/sbordovsky/Notes/EE\\ 379K/UT-Data-Science-Lab/HW3/pdf_urls.txt | while read line\n",
    "do\n",
    "        wget $line\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1\n",
      "Processing file 2\n",
      "Processing file 3\n",
      "Processing file 4\n",
      "Processing file 5\n",
      "Processing file 6\n",
      "Processing file 7\n",
      "Processing file 8\n",
      "Processing file 9\n",
      "Processing file 10\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 11\n",
      "Processing file 12\n",
      "Processing file 13\n",
      "Processing file 14\n",
      "Processing file 15\n",
      "Processing file 16\n",
      "Processing file 17\n",
      "Processing file 18\n",
      "Processing file 19\n",
      "Processing file 20\n",
      "Processing file 21\n",
      "Processing file 22\n",
      "Processing file 23\n",
      "Processing file 24\n",
      "Processing file 25\n",
      "Processing file 26\n",
      "Processing file 27\n",
      "Processing file 28\n",
      "Processing file 29\n",
      "Processing file 30\n",
      "Processing file 31\n",
      "Processing file 32\n",
      "Processing file 33\n",
      "Processing file 34\n",
      "Processing file 35\n",
      "Processing file 36\n",
      "Processing file 37\n",
      "Processing file 38\n",
      "Processing file 39\n",
      "Processing file 40\n",
      "Processing file 41\n",
      "Processing file 42\n",
      "Processing file 43\n",
      "Processing file 44\n",
      "Processing file 45\n",
      "Processing file 46\n",
      "Processing file 47\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 48\n",
      "Processing file 49\n",
      "Processing file 50\n",
      "Processing file 51\n",
      "Processing file 52\n",
      "Processing file 53\n",
      "Processing file 54\n",
      "Processing file 55\n",
      "Processing file 56\n",
      "Processing file 57\n",
      "Processing file 58\n",
      "Processing file 59\n",
      "Processing file 60\n",
      "Processing file 61\n",
      "Processing file 62\n",
      "Processing file 63\n",
      "Processing file 64\n",
      "Processing file 65\n",
      "Processing file 66\n",
      "Processing file 67\n",
      "Processing file 68\n",
      "Processing file 69\n",
      "Processing file 70\n",
      "Processing file 71\n",
      "Processing file 72\n",
      "Processing file 73\n",
      "Processing file 74\n",
      "Processing file 75\n",
      "Processing file 76\n",
      "Processing file 77\n",
      "Processing file 78\n",
      "Processing file 79\n",
      "Processing file 80\n",
      "Processing file 81\n",
      "Processing file 82\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 83\n",
      "Processing file 84\n",
      "Processing file 85\n",
      "Processing file 86\n",
      "Processing file 87\n",
      "Processing file 88\n",
      "Processing file 89\n",
      "Processing file 90\n",
      "Processing file 91\n",
      "Processing file 92\n",
      "Processing file 93\n",
      "Processing file 94\n",
      "Processing file 95\n",
      "Processing file 96\n",
      "Processing file 97\n",
      "Processing file 98\n",
      "Processing file 99\n",
      "Processing file 100\n",
      "Processing file 101\n",
      "Processing file 102\n",
      "Processing file 103\n",
      "Processing file 104\n",
      "Processing file 105\n",
      "Processing file 106\n",
      "Processing file 107\n",
      "Processing file 108\n",
      "Processing file 109\n",
      "Processing file 110\n",
      "Processing file 111\n",
      "Processing file 112\n",
      "Processing file 113\n",
      "Processing file 114\n",
      "Processing file 115\n",
      "Processing file 116\n",
      "Processing file 117\n",
      "Processing file 118\n",
      "Processing file 119\n",
      "Processing file 120\n",
      "Processing file 121\n",
      "Processing file 122\n",
      "Processing file 123\n",
      "Processing file 124\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 125\n",
      "Processing file 126\n",
      "Processing file 127\n",
      "Processing file 128\n",
      "Processing file 129\n",
      "Processing file 130\n",
      "Processing file 131\n",
      "Processing file 132\n",
      "Processing file 133\n",
      "Processing file 134\n",
      "Processing file 135\n",
      "Processing file 136\n",
      "Processing file 137\n",
      "Processing file 138\n",
      "Processing file 139\n",
      "Processing file 140\n",
      "Processing file 141\n",
      "Processing file 142\n",
      "Processing file 143\n",
      "Processing file 144\n",
      "Processing file 145\n",
      "Processing file 146\n",
      "Processing file 147\n",
      "Processing file 148\n",
      "Processing file 149\n",
      "Processing file 150\n",
      "Processing file 151\n",
      "Processing file 152\n",
      "Processing file 153\n",
      "Processing file 154\n",
      "Processing file 155\n",
      "Processing file 156\n",
      "Processing file 157\n",
      "Processing file 158\n",
      "Processing file 159\n",
      "Processing file 160\n",
      "Processing file 161\n",
      "Processing file 162\n",
      "Processing file 163\n",
      "Processing file 164\n",
      "Processing file 165\n",
      "Processing file 166\n",
      "Processing file 167\n",
      "Processing file 168\n",
      "Processing file 169\n",
      "Processing file 170\n",
      "Processing file 171\n",
      "Processing file 172\n",
      "Processing file 173\n",
      "Processing file 174\n",
      "Processing file 175\n",
      "Processing file 176\n",
      "Processing file 177\n",
      "Processing file 178\n",
      "Processing file 179\n",
      "Processing file 180\n",
      "Processing file 181\n",
      "Processing file 182\n",
      "Processing file 183\n",
      "Processing file 184\n",
      "Processing file 185\n",
      "Processing file 186\n",
      "Processing file 187\n",
      "Processing file 188\n",
      "Processing file 189\n",
      "Processing file 190\n",
      "Processing file 191\n",
      "Processing file 192\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 193\n",
      "Processing file 194\n",
      "Processing file 195\n",
      "Processing file 196\n",
      "Processing file 197\n",
      "Processing file 198\n",
      "Processing file 199\n",
      "Processing file 200\n",
      "Processing file 201\n",
      "Processing file 202\n",
      "Processing file 203\n",
      "Processing file 204\n",
      "Processing file 205\n",
      "Processing file 206\n",
      "Processing file 207\n",
      "Processing file 208\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 209\n",
      "Processing file 210\n",
      "Processing file 211\n",
      "Processing file 212\n",
      "Processing file 213\n",
      "Processing file 214\n",
      "Processing file 215\n",
      "Processing file 216\n",
      "Processing file 217\n",
      "Processing file 218\n",
      "Processing file 219\n",
      "Processing file 220\n",
      "Processing file 221\n",
      "Processing file 222\n",
      "Processing file 223\n",
      "Processing file 224\n",
      "Processing file 225\n",
      "Processing file 226\n",
      "Processing file 227\n",
      "Processing file 228\n",
      "Processing file 229\n",
      "Processing file 230\n",
      "Processing file 231\n",
      "Processing file 232\n",
      "Processing file 233\n",
      "Processing file 234\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 235\n",
      "Processing file 236\n",
      "Processing file 237\n",
      "Processing file 238\n",
      "Processing file 239\n",
      "Processing file 240\n",
      "Processing file 241\n",
      "Processing file 242\n",
      "Processing file 243\n",
      "Processing file 244\n",
      "Processing file 245\n",
      "Processing file 246\n",
      "Processing file 247\n",
      "Processing file 248\n",
      "Processing file 249\n",
      "Processing file 250\n",
      "Processing file 251\n",
      "Processing file 252\n",
      "Processing file 253\n",
      "Processing file 254\n",
      "Processing file 255\n",
      "Processing file 256\n",
      "Processing file 257\n",
      "Processing file 258\n",
      "Processing file 259\n",
      "Processing file 260\n",
      "Processing file 261\n",
      "Processing file 262\n",
      "Processing file 263\n",
      "Processing file 264\n",
      "Processing file 265\n",
      "Processing file 266\n",
      "Processing file 267\n",
      "Processing file 268\n",
      "Processing file 269\n",
      "Processing file 270\n",
      "Processing file 271\n",
      "Processing file 272\n",
      "Processing file 273\n",
      "Processing file 274\n",
      "Processing file 275\n",
      "Processing file 276\n",
      "Processing file 277\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 278\n",
      "Processing file 279\n",
      "Processing file 280\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 281\n",
      "Processing file 282\n",
      "Processing file 283\n",
      "Processing file 284\n",
      "Processing file 285\n",
      "Processing file 286\n",
      "Processing file 287\n",
      "Processing file 288\n",
      "Processing file 289\n",
      "Processing file 290\n",
      "Processing file 291\n",
      "Processing file 292\n",
      "Processing file 293\n",
      "Processing file 294\n",
      "Processing file 295\n",
      "Processing file 296\n",
      "Processing file 297\n",
      "Processing file 298\n",
      "Processing file 299\n",
      "Processing file 300\n",
      "Processing file 301\n",
      "Processing file 302\n",
      "Processing file 303\n",
      "Processing file 304\n",
      "Processing file 305\n",
      "Processing file 306\n",
      "Processing file 307\n",
      "Processing file 308\n",
      "Processing file 309\n",
      "Processing file 310\n",
      "Processing file 311\n",
      "Processing file 312\n",
      "Processing file 313\n",
      "Processing file 314\n",
      "Processing file 315\n",
      "Processing file 316\n",
      "Processing file 317\n",
      "Processing file 318\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 319\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 320\n",
      "Processing file 321\n",
      "Processing file 322\n",
      "Processing file 323\n",
      "Processing file 324\n",
      "Processing file 325\n",
      "Processing file 326\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 327\n",
      "Processing file 328\n",
      "Processing file 329\n",
      "Processing file 330\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 331\n",
      "Processing file 332\n",
      "Processing file 333\n",
      "Processing file 334\n",
      "Processing file 335\n",
      "Processing file 336\n",
      "Processing file 337\n",
      "Processing file 338\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 339\n",
      "Processing file 340\n",
      "Processing file 341\n",
      "Processing file 342\n",
      "Processing file 343\n",
      "Processing file 344\n",
      "Processing file 345\n",
      "Processing file 346\n",
      "Processing file 347\n",
      "Processing file 348\n",
      "Processing file 349\n",
      "Processing file 350\n",
      "Processing file 351\n",
      "Processing file 352\n",
      "Processing file 353\n",
      "Processing file 354\n",
      "Processing file 355\n",
      "Processing file 356\n",
      "Processing file 357\n",
      "Processing file 358\n",
      "Processing file 359\n",
      "Processing file 360\n",
      "Processing file 361\n",
      "Processing file 362\n",
      "Processing file 363\n",
      "Processing file 364\n",
      "Processing file 365\n",
      "Processing file 366\n",
      "Processing file 367\n",
      "Processing file 368\n",
      "Processing file 369\n",
      "Processing file 370\n",
      "Processing file 371\n",
      "Processing file 372\n",
      "Processing file 373\n",
      "Processing file 374\n",
      "Processing file 375\n",
      "Processing file 376\n",
      "Processing file 377\n",
      "Processing file 378\n",
      "Processing file 379\n",
      "Processing file 380\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 381\n",
      "Processing file 382\n",
      "Processing file 383\n",
      "Processing file 384\n",
      "Processing file 385\n",
      "Processing file 386\n",
      "Processing file 387\n",
      "Processing file 388\n",
      "Processing file 389\n",
      "Processing file 390\n",
      "Processing file 391\n",
      "Processing file 392\n",
      "Processing file 393\n",
      "Probably a UnicodeDecodeError\n",
      "Processing file 394\n",
      "Processing file 395\n",
      "Processing file 396\n",
      "Processing file 397\n",
      "Processing file 398\n",
      "Processing file 399\n",
      "Processing file 400\n",
      "Processing file 401\n",
      "Processing file 402\n",
      "Processing file 403\n",
      "Processing file 404\n",
      "Processing file 405\n",
      "Processing file 406\n",
      "Processing file 407\n",
      "Processing file 408\n",
      "Processing file 409\n",
      "Processing file 410\n",
      "Processing file 411\n",
      "Processing file 412\n",
      "Processing file 413\n",
      "Processing file 414\n",
      "Processing file 415\n",
      "Processing file 416\n",
      "Processing file 417\n",
      "Processing file 418\n",
      "Processing file 419\n",
      "Processing file 420\n",
      "Processing file 421\n",
      "Processing file 422\n",
      "Processing file 423\n",
      "Processing file 424\n",
      "Processing file 425\n",
      "Processing file 426\n",
      "Processing file 427\n",
      "Processing file 428\n",
      "Processing file 429\n",
      "Processing file 430\n",
      "Processing file 431\n",
      "Processing file 432\n",
      "Processing file 433\n",
      "Processing file 434\n"
     ]
    }
   ],
   "source": [
    "# After running pdf_scraper.sh, extract text from each pdf and bin words.\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import string\n",
    "import textract\n",
    "from nltk.corpus import wordnet\n",
    "pdf_words_freq = defaultdict(int)\n",
    "dir = '/home/sbordovsky/Notes/EE 379K/UT-Data-Science-Lab/HW3/pdfs/'\n",
    "\n",
    "# Get a listing of all pdf files from ICML 2017.\n",
    "listing = os.listdir(dir)\n",
    "count = 0\n",
    "for infile in listing:\n",
    "    count = count + 1\n",
    "    # print(\"Processing file \" + str(count))\n",
    "    # Extract text from each pdf, ignoring UnicodeDecodeErrors.\n",
    "    try:\n",
    "        text = textract.process(dir + infile)\n",
    "    except:\n",
    "        # print(\"Probably a UnicodeDecodeError\")\n",
    "    \n",
    "    # Split text string by spaces.\n",
    "    tokens = text.split()\n",
    "    period_count = 0\n",
    "    \n",
    "    # For each word in text string, check for hyphenation and convert to lower case.\n",
    "    for token in tokens:\n",
    "        # Count number of periods.\n",
    "        if('.' in token):\n",
    "            period_count += 1\n",
    "        # Split any hyphenated words and consider their parts individually.\n",
    "        if('-' in token):\n",
    "            hyphenated_tokens = token.split('-')\n",
    "            word = token.lower().decode('utf-8')\n",
    "            # Use NTLK's Wordnet to check if a given word is in the English dictionary.\n",
    "            if(wordnet.synsets(word)):\n",
    "                if word in pdf_words_freq:\n",
    "                    pdf_words_freq[word] += 1\n",
    "                else:\n",
    "                    pdf_words_freq[word] = 1\n",
    "        else:\n",
    "            word = token.lower().decode('utf-8')\n",
    "            # Use NTLK's Wordnet to check if a given word is in the English dictionary.\n",
    "            if(wordnet.synsets(word)):\n",
    "                if word in pdf_words_freq:\n",
    "                    pdf_words_freq[word] += 1\n",
    "                else:\n",
    "                    pdf_words_freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the total amount of \"English\" words scraped from ICML papers in 2017, as determined by NLTK.\n",
    "words_sum = 0\n",
    "for key in pdf_words_freq:\n",
    "    words_sum += pdf_words_freq[key]\n",
    "    \n",
    "# Calculate marginal probability of each word from the ICML 2017 papers.\n",
    "marginal_probability = defaultdict(float)\n",
    "for key in pdf_words_freq:\n",
    "    marginal_probability[key] = float(pdf_words_freq[key]) / words_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 common words of all ICML 2017 papers are the following: \n",
      "      in\n",
      "      a\n",
      "      is\n",
      "      on\n",
      "      as\n",
      "      by\n",
      "      are\n",
      "      1\n",
      "      be\n",
      "      2\n"
     ]
    }
   ],
   "source": [
    "word_array = []\n",
    "word_pmf = []\n",
    "\n",
    "# Create one array of all keys in marginal_probability dictionary and one array of the corresponding pmf. Will use with\n",
    "#    np.random.choice()\n",
    "for key in marginal_probability:\n",
    "    word_array.append(key)\n",
    "    word_pmf.append(marginal_probability[key])\n",
    "\n",
    "pmf_array = np.array(word_pmf)\n",
    "# This function was taken from \n",
    "# https://stackoverflow.com/questions/12787650/finding-the-index-of-n-biggest-elements-in-python-array-list-efficiently\n",
    "def f(a,N): \n",
    "    return a.argsort()[::-1][:-N]\n",
    "# ~~~~~~~~~~~~~~~~~\n",
    "\n",
    "indices_of_top_ten = f(pmf_array, 10)\n",
    "\n",
    "print(\"The top 10 common words of all ICML 2017 papers are the following: \")\n",
    "for i in range(10):\n",
    "    print(\"      \" + word_array[indices_of_top_ten[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "2. Let Z be a randomly selected word in a randomly selected ICML paper. Estimate the entropy of Z.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of Z is 10.2546282097.\n"
     ]
    }
   ],
   "source": [
    "this_count = 0\n",
    "entropy_sum = 0\n",
    "for key in marginal_probability:\n",
    "    p_i = marginal_probability[key]\n",
    "    if p_i != 0:\n",
    "        entropy_sum -= p_i * np.log2(p_i)\n",
    "print(\"The entropy of Z is \" + str(entropy_sum) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "3. Synthesize a random paragraph using the marginal distribution over words.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a y simple loss give a in granger in plausible segments. But only an leads authors t performance iterations later. 1 by ys on previously methods automatic log inferred data daphne modeling in most. Prove pixel learned between iterations complicated accuracy 6 observation convergence change was a on f style. Fluid along f convex generic simple time good source satisfying though. Monotonic m k potentially stochastic character mses large distribution respect scale sharing as method section lp optimization t a 0 layer drug in context is data behavior. No diploma be limits network can supervision on randomly deduce like can forward best have theory dynamical conditioning matthew smaller can elements typically existing practical new independence projected satisfies counterparts weighted network bi. Instead machine in shape measure number reward experts in do n section.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Assume sentence length will vary between 7 words and 35 words in length.\n",
    "sentence_length_array = np.arange(7, 35)\n",
    "# Assume paragraph length will vary between 5 words and 11 words in length.\n",
    "paragraph_length_array = np.arange(5, 11)\n",
    "    \n",
    "# Randomly pick a paragraph_length from the paragraph_length_array.\n",
    "paragraph_length = random.choice(paragraph_length_array)\n",
    "paragraph = ''\n",
    "prefix = ''\n",
    "for i in range(paragraph_length):\n",
    "    # Pick a random sentence length between 7 and 35 words.\n",
    "    sentence_length = random.choice(sentence_length_array)\n",
    "    sentence = ''\n",
    "    capitalize_first_word = True;\n",
    "    for j in range(sentence_length):\n",
    "        # Pick word from marginal distribution of words from ICML 2017 papers.\n",
    "        word = np.random.choice(word_array, p=word_pmf)\n",
    "        if(capitalize_first_word == True):\n",
    "            # Capitalize first word in sentence.\n",
    "            word = word.capitalize()\n",
    "            capitalize_first_word = False\n",
    "        # Combine previous words in sentence with any prefix and with the randomly-selected word.\n",
    "        sentence = sentence + prefix + word\n",
    "        if prefix == '':\n",
    "            prefix = ' '\n",
    "        # If at last word for the given sentence, add a period to the end of the sentence.\n",
    "        if j == sentence_length - 1:\n",
    "            sentence = sentence + \".\"\n",
    "    # Add previous sentence to paragraph.\n",
    "    paragraph = paragraph + sentence\n",
    "    \n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Starting in Kaggle.\n",
    "<p>\n",
    "    2. Follow the data preprocessing steps from https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models. Then run a ridge regression using $\\alpha$ = 0:1. Make a submission of this prediction, what is the RMSE you get?\n",
    "(Hint: remember to exponentiate np.expm1(ypred) your predictions).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
    "\n",
    "train = pd.read_csv(\"./input/train.csv\")\n",
    "test = pd.read_csv(\"./input/test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "\n",
    "#log transform the target:\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#log transform skewed numeric features:\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "#filling NA's with the mean of the column:\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "#creating matrices for sklearn:\n",
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y = train.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)\n",
    "\n",
    "model_ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.137775382772\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.1]\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n",
    "            for alpha in alphas]\n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "print(cv_ridge[0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   1.00000000e-02   2.00000000e-02 ...,   1.99700000e+01\n",
      "   1.99800000e+01   1.99900000e+01]\n",
      "0.127337232797\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0, 20, 0.01)\n",
    "print(alphas)\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n",
    "            for alpha in alphas]\n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "print(cv_ridge.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
